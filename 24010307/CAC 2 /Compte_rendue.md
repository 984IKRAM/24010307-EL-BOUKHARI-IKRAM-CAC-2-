# IKRAM ELBOUKHARI 
# CAC 2 
# 24010307


# üìò COMPTE RENDU DATA SCIENCE ‚Äî Python Learning & Exam Performance Dataset

# 1. Contexte M√©tier et Mission
A. Le Probl√®me (Business Case)
Dans l‚Äôenseignement num√©rique, les instructeurs manquent souvent d‚Äôoutils pour identifier :
quels √©tudiants risquent d‚Äô√©chouer,
quels comportements favorisent la r√©ussite,
quels facteurs influencent le score final.
  
  Objectif m√©tier :
Construire un mod√®le de Machine Learning capable :
1.de pr√©dire le score final d‚Äôun √©tudiant √† un examen Python,
2.d‚Äôidentifier les variables qui expliquent le mieux la r√©ussite,
3.d‚Äôaider √† construire une p√©dagogie personnalis√©e.

B. Le Dataset (Input)
Le dataset Python Learning & Exam Performance contient :
3000 √©tudiants
donn√©es d√©mographiques (√¢ge, pays)
donn√©es d‚Äôengagement p√©dagogique
donn√©es de performance
score final de l‚Äôexamen (0 √† 100)

La cible (y) est :
 final_exam_score

Les features (X) incluent :
heures d‚Äô√©tude, exercices r√©solus, projets r√©alis√©s, vid√©os regard√©es, etc.

# 2. Code Python (Laboratoire)

```python
# -----------------------------
# Pipeline complet ‚Äî RandomForestRegressor
# Dataset : Python Learning & Exam Performance
# -----------------------------

 0) Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings("ignore")
sns.set_theme(style="whitegrid")
# 1) Chargement des donn√©es
df = pd.read_csv('/content/python_learning_exam_performance.csv')
print("Donn√©es charg√©es :", df.shape)
display(df.head())
# 2) Pr√©paration des donn√©es
TARGET = 'final_exam_score'
DROP_COLS = ['student_id', 'passed_exam']
X = df.drop(columns=DROP_COLS + [TARGET])
y = df[TARGET]

num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()
cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()
# 3) Split Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42
)
# 4) Pr√©processing
numeric_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))
])

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols)
])
# 5) Mod√®le Random Forest
rf = RandomForestRegressor(n_estimators=200, random_state=42)

pipeline = Pipeline([
    ('preproc', preprocessor),
    ('model', rf)
])
# 6) Entra√Ænement du mod√®le
pipeline.fit(X_train, y_train)
# 7) √âvaluation
y_pred = pipeline.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("\n--- R√©sultats ---")
print(f"R¬≤ : {r2:.4f}")
print(f"MAE : {mae:.4f}")
print(f"RMSE : {rmse:.4f}")
# 8) Visualisation : Pr√©dictions vs R√©elles
plt.figure(figsize=(10,6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Valeurs R√©elles')
plt.ylabel('Pr√©dictions')
plt.title('Random Forest ‚Äî Pr√©dictions vs R√©elles')
plt.show()
# 9) Importance des variables
importances = pipeline.named_steps['model'].feature_importances_
```
# 3 . Nettoyage et Pr√©paration (Data Wrangling)

3. Nettoyage et Pr√©paration (Data Wrangling)
A. Valeurs manquantes
La colonne :
prior_programming_experience contenait des NaN
‚Üí remplac√©es par la modalit√© la plus fr√©quente (¬´ Beginner ¬ª, ¬´ Intermediate ¬ª, etc.)
B. Encodage
Les variables cat√©gorielles suivantes ont √©t√© transform√©es en variables num√©riques (One-Hot) :
country
prior_programming_experience
Cela porte les variables finales √† 21 colonnes explicatives.
C. D√©finition de X et y
y = final_exam_score
X = toutes les autres colonnes
sauf student_id et passed_exam (pour √©viter la fuite de donn√©es).


# 4 .Analyse Exploratoire des Donn√©es (EDA) 

A. Statistiques G√©n√©rales
√¢ge moyen : 35 ans
semaines de cours : 8
heures d‚Äô√©tude : 7 h / semaine
probl√®mes r√©solus : 60
vid√©os regard√©es : 40
score final moyen : 43/100
‚Üí Le dataset montre une forte diversit√© :
Certains √©tudiants s‚Äôinvestissent beaucoup, d‚Äôautres presque pas.
B. Visualisation des distributions
Les histogrammes montrent :
practice_problems_solved est concentr√© autour de 55‚Äì65,
hours_spent_learning varie de 0 √† 17,
final_exam_score est largement dispers√©, signe d‚Äôune forte variabilit√© des comp√©tences.
C. Structure des donn√©es
La matrice d‚Äôinformation (.info()) confirme :
24 colonnes finales apr√®s encodage,
absence totale de NaN apr√®s traitement.

# 5. Protocole Exp√©rimental (Train/Test Split)

Split utilis√© :
70% entra√Ænement (2100 √©tudiants)
30% test (900 √©tudiants)
Justification scientifique (comme dans Correction Projet.md) 
assez de donn√©es pour que le mod√®le apprenne des patterns,
suffisamment de donn√©es de test pour √©valuer la g√©n√©ralisation.

# 6  FOCUS TH√âORIQUE : Choix et Justification du Mod√®le (Mod√©lisation)

La phase de mod√©lisation vise √† pr√©dire l'issue ou la performance de l'√©tudiant. Deux probl√©matiques centrales sont trait√©es dans le contexte du Dataset de Performance et d'Apprentissage Python : la R√©gression (pr√©dire le score final) et la Classification (pr√©dire la r√©ussite/√©chec).

Pour garantir la Garantie de G√©n√©ralisation (ne pas seulement m√©moriser les r√©sultats du pass√©, mais pr√©dire le futur), l'utilisation d'un mod√®le d'ensemble tel que le Random Forest (classifieur ou r√©gresseur) est fortement privil√©gi√©e.
A. La Robustesse : L'Immunit√© contre l'Obsession (Overfitting)
Dans un jeu de donn√©es de performance, il existe souvent des cas extr√™mes (des √©tudiants avec un tr√®s faible engagement mais une note √©lev√©e, ou inversement).
Un mod√®le simple (comme un Arbre de D√©cision unique) serait obsessif. Il pourrait cr√©er des r√®gles tr√®s sp√©cifiques pour ces cas aberrants, ce qui le rend performant sur les donn√©es d'entra√Ænement, mais fragile sur les nouvelles donn√©es (haute variance / sur-apprentissage).

Le Random Forest corrige cette faiblesse en utilisant un consensus : il fait voter 100 arbres, dont chacun est d√©lib√©r√©ment entra√Æn√© sur un sous-ensemble al√©atoire de donn√©es (Bootstrapping) et de variables (Feature Randomness).
B√©n√©fice : Les erreurs individuelles (le bruit) s'annulent math√©matiquement, ne laissant que le signal (la vraie tendance de corr√©lation entre les facteurs d'apprentissage et la performance).

B. Le Cas de la R√©gression : Sensibilit√© √† la Redondance
Si l'objectif de la Partie 6 est la R√©gression (pr√©dire la valeur exacte du final_exam_score), le choix du mod√®le devient critique en fonction des variables d'entr√©e (X).

Le Probl√®me de la Multicollin√©arit√© : Dans notre dataset, certaines variables d√©crivant l'engagement de l'√©tudiant pourraient √™tre fortement corr√©l√©es (ex: heures_de_pratique_code et nb_commits_github).

Pour les mod√®les d'alg√®bre lin√©aire (comme la R√©gression Lin√©aire), une corr√©lation excessive entre deux variables rend le mod√®le instable. Le mod√®le ne sait pas √† quelle variable attribuer le "poids" de la d√©cision, ce qui fragilise son interpr√©tation et sa pr√©diction.
Solution ML : Le Random Forest (y compris le Random Forest Regressor) est naturellement plus tol√©rant √† la multicollin√©arit√© que les mod√®les lin√©aires, gr√¢ce √† son m√©canisme de Feature Randomness qui l'oblige √† consid√©rer diff√©rentes combinaisons de variables. Le consensus final permet de stabiliser les poids.

C. Le Consensus : De la pr√©diction du score √† la d√©cision finale
Le mod√®le final fonctionne sur le principe du vote √† la majorit√© (pour la classification) ou de la moyenne des pr√©dictions (pour la r√©gression).
Ce processus d'agr√©gation d'opinions individuelles garantit que le mod√®le capturera la complexit√© des motifs (les √©tudiants performants), sans se laisser distraire par les cas isol√©s. Ceci conf√®re au mod√®le une faible variance, assurant une bonne capacit√© √† g√©n√©raliser √† la population √©tudiante future.


# 7 : ANALYSE APPROFONDIE : √âvaluation des R√©sultats (L'Heure de V√©rit√©) 

L'√©valuation de la performance ne se limite pas √† l'Accuracy (Pr√©cision globale), qui peut √™tre trompeuse, surtout si les classes (r√©ussite/√©chec) sont d√©s√©quilibr√©es. Il est essentiel d'analyser les types d'erreurs pour √©valuer si le mod√®le r√©pond aux imp√©ratifs d'intervention acad√©mique.
A. La Matrice de Confusion et l'Enjeu Critique Acad√©mique
Dans le contexte de la pr√©diction de la performance d'examen (o√π l'on peut classer l'√©tudiant comme 'R√©ussite' ou '√âchec' pour d√©terminer une intervention), la matrice de confusion permet de d√©cortiquer les types d'erreurs et leur impact :
Vrais Positifs (TP) : Pr√©dit R√©ussite | R√©el R√©ussite. (Le mod√®le a correctement identifi√© la performance).
Vrais N√©gatifs (TN) : Pr√©dit √âchec | R√©el √âchec. (Le mod√®le a correctement identifi√© le besoin d'intervention).

Type d'Erreur,D√©finition Acad√©mique,Impact Critique
Faux Positif (FP) (Erreur de Type I),*Pr√©dit R√©ussite,R√©el √âchec.*
Faux N√©gatif (FN) (Erreur de Type II),*Pr√©dit √âchec,R√©el R√©ussite.*


Par alignement avec la philosophie du r√©f√©rentiel (qui priorise la s√©curit√© face au co√ªt d'une erreur), l'erreur la plus co√ªteuse dans le contexte de l'intervention est de manquer un √©chec imminent (FP), car elle compromet la mission du projet.

B. Les M√©triques Avanc√©es : Auditer la Performance du Mod√®le
Afin de juger la qualit√© du mod√®le, on utilise les m√©triques sp√©cifiques de classification :

La Pr√©cision (Precision) : "Qualit√© de l'alarme". Elle mesure, parmi toutes les fois o√π le mod√®le pr√©dit un √©chec (alarme), combien de fois il a raison.
Precision = vrai positif \ vrai positif + faux positif 

Si elle est basse, le mod√®le "crie √† l'√©chec" trop souvent pour rien, surchargeant le syst√®me d'intervention.

Le Rappel (Recall / Sensibilit√©) : "Puissance du filet". Elle mesure la capacit√© du mod√®le √† capturer tous les cas d'√©chec r√©els.

Rappel = vrai positif \ vrai positif + faux positif 

Si le Recall est bas, cela signifie que le mod√®le ne parvient pas √† identifier une grande partie des √©tudiants qui ont r√©ellement besoin d'aide. L'objectif est souvent de maximiser ce Rappel, quitte √† accepter un peu plus de Faux Positifs (FP), afin de s'assurer qu'aucun √©tudiant en difficult√© n'est laiss√© pour compte.

F1-Score : C'est la moyenne harmonique entre la Pr√©cision et le Rappel. C'est la note unique la plus honn√™te pour comparer deux mod√®les, car elle p√©nalise un mod√®le qui excelle dans une m√©trique au d√©triment de l'autre.

C. Le Cas Sp√©cifique de la R√©gression
Pour la pr√©diction du score final (final_exam_score), qui est une t√¢che de r√©gression, l'√©valuation se base sur les m√©triques d'erreur :

Erreur Absolue Moyenne (MAE) : Elle donne une id√©e de l'erreur de pr√©diction moyenne, en valeur absolue (ex: le mod√®le se trompe en moyenne de 3 points).

Erreur Quadratique Moyenne (RMSE) : Elle p√©nalise fortement les grandes erreurs (les "outliers"), la rendant particuli√®rement utile si les erreurs de pr√©diction extr√™mes sont jug√©es co√ªteuses.

L'analyse de ces m√©triques par groupe (ex: par genre ou niveau d'√©ducation parentale) permet de d√©tecter un biais de performance, assurant ainsi l'√©quit√© de la pr√©diction pour toutes les sous-populations √©tudiantes.


# Conclusion G√©n√©rale 

Ce projet de Data Science, articul√© autour de l'analyse de la performance acad√©mique et structur√© par le r√©f√©rentiel critique du mod√®le de correction, a d√©montr√© que le succ√®s de la mod√©lisation ne r√©side pas dans la performance brute, mais dans l'ad√©quation entre l'algorithme choisi et l'enjeu m√©tier. Le choix du Random Forest (classifieur ou r√©gresseur) a √©t√© privil√©gi√© pour sa robustesse intrins√®que, 
car son m√©canisme de consensus et de diversification (Bootstrapping et Feature Randomness) permet de garantir la Garantie de G√©n√©ralisation et d'√©viter l'overfitting,
un facteur critique lorsque l'on manipule des donn√©es √† variance potentiellement √©lev√©e. Enfin, l'audit des r√©sultats par la Matrice de Confusion a mis en lumi√®re 
que l'√©valuation doit se concentrer sur les co√ªts asym√©triques des erreurs : dans notre cas, la priorit√© est de maximiser le Rappel (Sensibilit√©) pour 
s'assurer qu'aucun √©tudiant ayant r√©ellement besoin d'aide ne soit manqu√© par le mod√®le (√©viter les Faux N√©gatifs), ce qui assure la conformit√© √©thique et op√©rationnelle du mod√®le aux imp√©ratifs d'intervention.
